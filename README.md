## YouTube ğŸ“º Data Mining & NLP: Automated ğŸ“ˆ ETL for scraping, processing, text analytics, insights via Aws, Python, Firebase & Streamlit

### âœ¨ Demo app Link: &nbsp; <a href="https://yt-comments-sentiment-analyzer.streamlit.app/"><img src="https://github.com/KunalAnand2907/Youtube_DataMining_Analysis-End-End-Data-Engineering-Data-Science-Project/assets/46574881/4f490ebc-1c79-4b10-b869-cf319682598e"></a>

#

### ğŸ“Œ Dataset Overview:

This project utilizes data collected through two different sources:

<ul> <li> <b>1. Trending YouTube Video Statistics</b> â€” sourced from Kaggle <br>
 
<b>ğŸ”— Dataset Link:</b> [YouTube Trending Video Dataset](https://www.kaggle.com/datasets/datasnaek/youtube-new) 
 <ul> <li><b>CSV Files:</b> Each regionâ€™s dataset is stored separately and contains attributes such as <i>video title, video ID, category ID (1â€“41), channel title, publish time, tags, views, likes, dislikes, description,</i> and <i>comment count.</i></li> <li><b>JSON Files:</b> Structured in key-value pairs for each region, with an <code>id</code> acting as the primary key. Each record includes a <code>snippet</code> object containing nested fields like <code>channelId</code> and <code>categoryName</code>â€”categorizing videos into domains such as <i>Music, Sports, Technology, Entertainment,</i> and more.</li> </ul> </li> 
<br>
<li> <b>2. YouTube Data Scraped via YouTube Data API</b> â€” collected dynamically based on user-defined search terms. For this project, data was extracted using the keyword <b>"Data Science"</b>. </li> </ul>

#

### ğŸš€ Key Features:

-- In Making

#

### ğŸ“¦ Project Structure: 

-- In Making

#

### ğŸ¯ The Project is divided into 3 Parts:

---

 **1ï¸âƒ£ Real-Time Auth & Interactive User Posts with Firebase + Firestore on Streamlit App**

ğŸ”¥ This module powers a secure, real-time user experience using Firebase Authentication and Firestore Database, seamlessly integrated into the Streamlit App â‡¢ [Visit Streamlit_App Folder](https://github.com/KunalAnand2907/Youtube-Data-Mining-Analytics-End-End-Data-Engineering-Data-Science-Project/tree/master/Part1_Streamlit_App)

**âš™ï¸ Key Functionalities**

1ï¸. User Authentication & Authorization:
 <ul> 
 <li> New users can Sign Up using an email, password, and unique username, and later Log In securely via Firebase Authentication. </li>
 <li> Each userâ€™s credentials are validated to ensure uniqueness of both email and username, enabling a personalized session and secure data access. </li>
 </ul>
 
2ï¸. Dynamic Post Management via Firestore:
<ul> 
 <li> Authenticated users can create, view, and delete there own posts in a collaborative Post Notes Space & can view other's & owner post on Home Page. </li>
 <li> All posts are stored in Firestore (NoSQL Real-Time Database) under each userâ€™s unique document ID. </li>
 <li> Data is organized into collections per user, where posts are ordered by User_ID, ensuring fast retrieval and real-time updates directly reflected in the Streamlit interface. </li>
 </ul>

---

**2ï¸âƒ£ Analytic Platform with end - end ETL Data Pipeline For Trending YouTube Video Statistics via AWS** â‡¢ [Visit ETL_AWS Folder]

#### ğŸ“š Overview

A fully automated ETL Data Pipeline designed to extract, clean, and analyze YouTube trending video data from multiple regions/Countries â€” leveraging AWS services for scalable processing, schema handling, and real-time analytics.
The platform securely handles structured and semi-structured data (JSON, CSV, Parquet) to uncover insights across key metrics such as ideo categories (Entertainment, Science and fiction, etc), Video Tile and desc, Channel Name & ID, Likes & Description, comments & its count.

#### ğŸ§© Workflow Summary:

Built from scratch using AWS serverless architecture:

1ï¸âƒ£ Raw Data Ingestion â†’ Uploaded regional/Diff. Countries JSON and Partitioned CSV files based on Regional Tag (Ind.csv, eng.csv, etc.) into 2 S3 Raw Bucket. <br>
2ï¸âƒ£ Schema Discovery â†’ Created AWS Glue Catalog; handled nested JSON struct arrays via preprocessing. <br>
3ï¸âƒ£ Automated Cleansing â†’ Used AWS Lambda triggers on S3 PUT events to clean and append data into S3 (Cleansed Bucket). <br>
4ï¸âƒ£ Data Transformation â†’ Executed PySpark Glue Jobs with Glue Bookmarks for schema normalization, null handling, and outlier's treatment. <br>
5ï¸âƒ£ Data Integration/ Combining â†’ Performed inner joins on category IDs and stored curated datasets into S3 (Analytic Bucket). <br>
6ï¸âƒ£ Query & Analysis â†’ Queried partitioned data with Athena, storing query output & metadata into 2 different named as : a.) S3 (Athena Output Bucket), b.) S3 (Logs Athena Query Bucket) <br>
7ï¸âƒ£ Visualization & Insights â†’ Leveraged QuickSight Dashboards to track KPIs like mentioned below, & create diff. data driven dashboards including various Graphs & Charts. <br>

**â” Top/Bottom 10 trending videos by region**

**â” Category-wise views, likes, and comment counts**

**â” Global vs. regional performance trends**

### [ğŸ”— View Detailed Architecture & Workflow â†’](https://drive.google.com/drive/u/0/folders/19idDsEe7xafxWRVmEaYkRSfAbbYlmCbb)

#

### ğŸ—ï¸ Architecture Diagram

![ETL_AWS](https://github.com/user-attachments/assets/eafba805-cdb6-400d-a208-f061ee1ee643)

#

ğŸ“š **Tech Stack:**

â” **Languages --** SQL, Python3 |
â” **File Formats --** Json, Parquet, Csv

â” **Services:**
<ul>
<li><b>S3:</b> Amazon S3 is an object storage service that provides manufacturing scalability, data availability, security, and performance.
<li><b>IAM (Users, Groups & Role):</b> This is nothing but identity and access management which enables us to manage access to AWS services and resources securely.
<li><b>QuickSight:</b> Amazon QuickSight is a scalable, serverless, embeddable, machine learning-powered business intelligence (BI) service built for the cloud.
<li><b>Glue (Crawler, Studio & Glue Catlog):</b> A serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development.
<li><b>Lambda:</b> Lambda is a computing service that allows programmers to run code without creating or managing servers.
<li><b>Athena:</b> Athena is an interactive query service for S3 in which there is no need to load data it stays in S3.
<li><b>SNS:</b> A distributedÂ publish/subscribeÂ solution used for application-to-application (A2A) and application-to-person (A2P) communication. SNS topics are used to enable communication: producers publish messages to topics, and consumers subscribe to these topics to receive messages. You can deliver messages to various types of subscribers, such as AWS SQS queues, AWS Lambda functions, and HTTP endpoints. You can also use SNS to send SMS messages, email, and push notifications to end-user devices.
<li><b>Cloudwatch & Logs:</b> It enables you to monitor your complete stack (applications, infrastructure, network, and services) and use alarms, logs, and events data to take automated actions and reduce mean time to resolution (MTTR). This frees up important resources and allows you to focus on building applications and business value.
</ul>

---
**3ï¸âƒ£ YouTube Data Scraping, Transformation, Preprocessing, EDA, and NLP-based Text Mining with RAG â‡¢** [ Visit DataScrapping_Viz_Nlp_Tasks Folder ]

This Section is further divided into 2 Parts:

â–¶ï¸ **Scrapping Youtube data, Pre-Processing, Wrangling, and Visualizing Data via different Charts & Graphs.**

â‡¢ Youtube data is scrapped by using Youtube Data API according to search Data Science & we have Attributes such as:
<ul>
<li> <b> For Different Channel's Data:</b>
Attributes s.a. Channel Name, subscribers, Total Views, Total Videos, Playlist ID
<li> <b> For each Channel Data:</b>
Attributes s.a. Video title, Video id, Video Description, Published date, Likes, Dislikes, Views, Comments
<li> After that I Performed Data Processing, Wrangling operations on the Data mentioned above & then Performed EDA (uni, Bi, Tri) Variate Data Visualization on the Cleaned Data
</ul>

ğŸ“„ğŸ” **Text Mining - Performing NLP Tasks on the Youtube Statistics Data**

â‡¢ Predict the Category_id (Y) Based on Video_Title (X)
<ul>
<li> <b>Text Pre Processing 1 --</b> Tokenize [Sentences, Words] â¡ï¸ Text Cleaning { Regex - remove Punctuations, Special Chars, extra white spaces} â¡ï¸ Remove StopWords â¡ï¸ Stemming & Lemmatization â¡ï¸ POS Tagging â¡ï¸ NER
<li> <b>Text Pre Processing 2 --</b> Word Embeddings { Ml Embeddings - BOW, TF-IDF, Word2Vec, DL Embeddings - LSTM/ Bi - Dir LSTM with Word Embeddings }
<li> Model Building & Training
<li> Evaluation and tuning of Parameters
<li> Comparing all the Word Embeddings Model
<li> Topic Categorization: At the end map the Category_id to 16 different Category Name s.a [ Film & Animations, Sports, Science & Technology, Entertainment, Music, News, Daily Vlogs ] etc.
</ul>

# 
### Watch the Below Video for working of the App

[![Main_Page_ss](https://github.com/KunalAnand2907/Youtube_DataMining_Analysis-End-End-Data-Engineering-Data-Science-Project/assets/46574881/b480838d-991b-4387-994c-bb3c90e9a081)](https://youtu.be/GaeUzR9szVM)
